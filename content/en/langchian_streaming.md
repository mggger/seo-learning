---
title: "Langchain LLM Streaming "
date: 2023-10-18
draft: false
description: How to Perform Real-Time Processing of Langchain LLM's Tokens
categories: ["LangChain", "ChatGPT", "AI"]
tags: ["LangChain", "AI"]
faq:
  - question: "What is Langchain LLM Streaming?"
    answer: "Langchain LLM Streaming is a feature that allows real-time processing of tokens generated by Large Language Models (LLMs) through a callback mechanism."
  - question: "What are the types of IO processing supported by Langchain?"
    answer: "Langchain supports both synchronous and asynchronous IO processing for token output, accommodating different application requirements."
  - question: "How does the StreamingStdOutCallbackHandler work?"
    answer: "StreamingStdOutCallbackHandler provides real-time printing of LLM-generated tokens to the terminal using the `on_llm_new_token` method."
  - question: "What is the advantage of using AsyncIteratorCallbackHandler?"
    answer: "AsyncIteratorCallbackHandler allows for asynchronous processing of Langchain LLM tokens, making it ideal for scalable and non-blocking operations."
  - question: "Where can I learn more about AI and Langchain applications?"
    answer: "You can learn more about AI and its applications in Langchain by visiting the AI category page on our website."
---

# Langchain LLM Streaming
Langchain offers the capability to perform real-time processing of tokens generated by Large Language Models (LLMs) through a `callback` mechanism, revolutionizing the way we interact with AI.

![langchan_stream](/img/langchain_stream.png)

## Introduction to Langchain Streaming
Langchain's innovative approach allows users to process LLM tokens in real-time, enhancing efficiency and user experience. This feature is particularly valuable in applications requiring immediate token processing, such as chatbots or real-time data analysis.

### Synchronous vs. Asynchronous Processing
Langchain supports both and asynchronous IO for token output, catering to different application needs. The `StreamingStdOutCallbackHandler` handles synchronous processing, while `AsyncIteratorCallbackHandler` manages asynchronous operations.

## StreamingStdOutCallbackHandler
### Understanding the StreamingStdOutCallbackHandler
The `StreamingStdOutCallbackHandler` provides real-time printing of LLM-generated tokens to the terminal, as shown in the following example:

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    HumanMessage,
)
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = chat([HumanMessage(content="Write me a song about sparkling water.")])
```

This handler uses the on_llm_new_token method to print tokens as they are generated.

```python
class StreamingStdOutCallbackHandler(BaseCallbackHandler):
	...
	
	def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
		"""Run on new LLM token. Only available when streaming is enabled."""
		sys.stdout.write(token)
		sys.stdout.flush()

```

## AsyncIteratorCallbackHandler
### Asynchronous Processing with AsyncIteratorCallbackHandler
For applications requiring non-blocking operations, the `AsyncIteratorCallbackHandler` provides an efficient solution:

```python
import asyncio

from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
import os
import openai
from typing import AsyncIterable, Awaitable

# Async chat function for interacting with the ChatOpenAI model
async def async_chat(message) -> AsyncIterable[str]:
    # Create an asynchronous callback handler
    callback = AsyncIteratorCallbackHandler()
    # Create a ChatOpenAI instance with streaming mode enabled, using the callback handler and a temperature parameter
    chat = ChatOpenAI(streaming=True, callbacks=[callback], temperature=0)

    # Define an asynchronous function to wrap another asynchronous function and signal completion or exceptions using an event
    async def wrap_done(fn: Awaitable, event: asyncio.Event):
        try:
            await fn  # Wait for the provided asynchronous function to complete
        except Exception as e:
            # TODO: Handle exceptions - here, we simply print the exception information
            print(f"Caught exception: {e}")
        finally:
            event.set()  # Set the event to indicate completion

    # Create a task to perform message generation with ChatOpenAI and monitor the completion event of the callback handler
    task = asyncio.create_task(wrap_done(chat.agenerate(messages=[[HumanMessage(content=message)]], callback.done))

    # Iterate asynchronously to obtain tokens from the callback handler
    async for token in callback.aiter():
        yield f"{token}"  # Convert tokens to strings and yield them

    await task  # Wait for the task to complete

# Async function to print tokens generated by ChatOpenAI
async def async_print():
    message = "Write me a song about sparkling water."
    async for token in async_chat(message):
        print(token, end='')

if __name__ == '__main__':
    asyncio.run(async_print())  # Run the async_print function as the entry point
```

This code allows asynchronous processing of Langchain LLM tokens and printing the results, ideal for scalable applications.



## Conclusion
Langchain's streaming capabilities are a significant advancement in real-time LLM token processing. Whether you choose synchronous or asynchronous methods, Langchain streamlines your AI interactions, making them more efficient and responsive.

For more insights on AI and its applications, visit our [AI category page](/categories/ai/).




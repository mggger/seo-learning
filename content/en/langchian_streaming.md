---
title: "Langchain LLM Streaming with OpenAI Streams "
date: 2023-10-18
draft: false
description: Explore real-time processing of Langchain LLM's tokens with OpenAI streaming capabilities.
categories: ["LangChain", "ChatGPT", "AI", "OpenAI Streaming"]
tags: ["LangChain", "AI", "OpenAI Stream", "StreamingLLM"]
toc: true
faq:
  - question: "What is Langchain LLM Streaming?"
    answer: "Langchain LLM Streaming is a feature that allows real-time processing of tokens generated by Large Language Models (LLMs) through a callback mechanism."
  - question: "What are the types of IO processing supported by Langchain?"
    answer: "Langchain supports both synchronous and asynchronous IO processing for token output, accommodating different application requirements."
  - question: "How does the StreamingStdOutCallbackHandler work?"
    answer: "StreamingStdOutCallbackHandler provides real-time printing of LLM-generated tokens to the terminal using the `on_llm_new_token` method."
  - question: "What is the advantage of using AsyncIteratorCallbackHandler?"
    answer: "AsyncIteratorCallbackHandler allows for asynchronous processing of Langchain LLM tokens, making it ideal for scalable and non-blocking operations."
  - question: "Where can I learn more about AI and Langchain applications?"
    answer: "You can learn more about AI and its applications in Langchain by visiting the AI category page on our website."
---

# Streamlining AI Interactions with Langchain LLM Streaming and OpenAI

Langchain offers the capability to perform real-time processing of tokens generated by Large Language Models (LLMs) through a `callback` mechanism, integrating OpenAI streaming technology to revolutionize AI interactions.

![langchan_stream](/img/langchain_stream.png)

## Getting Started with Langchain and OpenAI Streaming

Langchain's real-time streaming capability, powered by OpenAI technology, significantly enhances efficiency and user experience in applications like chatbots or data analysis.

### Choosing Between Synchronous and Asynchronous Processing

Langchain supports both synchronous and asynchronous IO for token output, catering to different application needs. The `StreamingStdOutCallbackHandler` handles synchronous processing, while `AsyncIteratorCallbackHandler` manages asynchronous operations in the realm of OpenAI streams.


## Real-Time Responses with StreamingStdOutCallbackHandler
### Mastering StreamingStdOutCallbackHandler
The `StreamingStdOutCallbackHandler` provides real-time printing of LLM-generated tokens to the terminal, enhancing the openai stream response capabilities.

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    HumanMessage,
)
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = chat([HumanMessage(content="Write me a song about sparkling water.")])
```

This handler uses the `on_llm_new_token` method to print tokens as they are generated, exemplifying the efficiency of StreamLLM.

```python
class StreamingStdOutCallbackHandler(BaseCallbackHandler):
	...
	
	def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
		"""Run on new LLM token. Only available when streaming is enabled."""
		sys.stdout.write(token)
		sys.stdout.flush()

```

## Leveraging AsyncIteratorCallbackHandler for Asynchronous Tasks
### Asynchronous Efficiency with AsyncIteratorCallbackHandler
For applications requiring non-blocking operations, the `AsyncIteratorCallbackHandler` provides an efficient solution for managing OpenAI-streams.


```python
import asyncio

from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
import os
import openai
from typing import AsyncIterable, Awaitable

# Async chat function for interacting with the ChatOpenAI model
async def async_chat(message) -> AsyncIterable[str]:
    # Create an asynchronous callback handler
    callback = AsyncIteratorCallbackHandler()
    # Create a ChatOpenAI instance with streaming mode enabled, using the callback handler and a temperature parameter
    chat = ChatOpenAI(streaming=True, callbacks=[callback], temperature=0)

    # Define an asynchronous function to wrap another asynchronous function and signal completion or exceptions using an event
    async def wrap_done(fn: Awaitable, event: asyncio.Event):
        try:
            await fn  # Wait for the provided asynchronous function to complete
        except Exception as e:
            # TODO: Handle exceptions - here, we simply print the exception information
            print(f"Caught exception: {e}")
        finally:
            event.set()  # Set the event to indicate completion

    # Create a task to perform message generation with ChatOpenAI and monitor the completion event of the callback handler
    task = asyncio.create_task(wrap_done(chat.agenerate(messages=[[HumanMessage(content=message)]], callback.done))

    # Iterate asynchronously to obtain tokens from the callback handler
    async for token in callback.aiter():
        yield f"{token}"  # Convert tokens to strings and yield them

    await task  # Wait for the task to complete

# Async function to print tokens generated by ChatOpenAI
async def async_print():
    message = "Write me a song about sparkling water."
    async for token in async_chat(message):
        print(token, end='')

if __name__ == '__main__':
    asyncio.run(async_print())  # Run the async_print function as the entry point
```

This code demonstrates the asynchronous processing of Langchain LLM tokens, making it ideal for scalable applications that utilize OpenAI stream response.



## Wrapping Up
Langchain's streaming capabilities, coupled with OpenAI streaming technology, mark a significant advancement in real-time LLM token processing. Whether you choose synchronous or asynchronous methods, Langchain and OpenAI streams streamline your AI interactions, making them more efficient and responsive.

For more insights on AI and its applications, including OpenAI-streams and StreamingLLM, visit our [AI category page](/categories/ai/).




---
title: GPT的架构 - 从0构建LLM (1)
date: 2024-08-07
draft: false
description: "本章节是从零构建LLM系列文章第一章，详细解释了GPT模型的架构"
categories: ["llm", "hive"]
tags: ["llm"]
toc: true
---

# GPT的架构

![GPT architecture](/img/gpt-architecture.png)


**GPT模型是一种计算机程序，它可以理解和生成文字，就像我们人类会说话和写字一样。**



## 模型的工作流程

1. **输入文本：** 首先，我们把要处理的文字（比如一句话或一段话）输入到模型中。

2. **词嵌入层：** 模型会把这些文字转换成一种计算机能理解的数字形式，这叫“词嵌入”。这就好像我们用字母拼出一个单词一样，计算机用数字来表示这些文字。

3. **位置嵌入层：** 模型还会考虑每个单词在句子中的位置，因为同样的单词在不同的位置可能有不同的意思。

4. **多个转换器模块：**
   - 这些模块负责处理和理解输入的文字。每个模块都有几个重要部分：
     - **注意力机制（Masked Multi-head Attention）：** 这部分就像我们阅读一篇文章时，会特别注意一些关键字一样。模型会关注输入文字中的重要部分。
     - **前馈神经网络（Feed Forward）：** 这部分就像一个过滤器，会进一步处理信息，使其变得更有用。
     - **规范化层（LayerNorm）：** 这部分会确保信息在处理过程中保持稳定，不会变得太乱。
     - **丢弃层（Dropout）：** 这部分会随机丢弃一些信息，防止模型过于依赖某些特定的信息，使其更具通用性。

5. **输出层：** 最后，处理完所有信息后，模型会给出一个结果，比如生成下一段文字。

## 示例

假设我们输入一句话：“今天的天气真好。”

1. **输入文本：** “今天的天气真好。”
2. **词嵌入层：** 把这句话转换成数字。
3. **位置嵌入层：** 记录每个词在句子中的位置。
4. **多个转换器模块：** 模型会处理这句话，注意到“天气”和“好”是关键字，并对信息进行多次处理。
5. **输出层：** 最后模型可能会生成下一句话，比如“我们去公园玩吧。”



## 大模型评测

MMLU=Measuring Massive Multitask language Understanding， 

以下是一个示例

```
input = ("1 + 1=?", "A. 2", "B. 3", "C. 4")

model_answer = model(input)

correct_answer = "C.4"
score += model_answer == correct_answer

total_score = score / num_examples * 100 %
```

